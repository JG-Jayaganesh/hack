{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "path = \"data/\"\n",
    "import importlib as imp\n",
    "import utils; imp.reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n",
      "Found 106 images belonging to 13 classes.\n",
      "Found 4 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n",
      "Found 106 images belonging to 13 classes.\n",
      "Found 4 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n",
      "Found 106 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/val.dat', val)\n",
    "save_array(path+'results/trn.dat', trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = load_array(path+'results/val.dat')\n",
    "trn = load_array(path+'results/trn.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(13, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "341/341 [==============================] - 49s - loss: 2.5330 - acc: 0.2493 - val_loss: 13.6907 - val_acc: 0.0566\n",
      "Epoch 2/2\n",
      "341/341 [==============================] - 46s - loss: 0.5723 - acc: 0.8475 - val_loss: 11.3116 - val_acc: 0.0755\n",
      "Epoch 1/4\n",
      "341/341 [==============================] - 48s - loss: 0.2728 - acc: 0.9619 - val_loss: 6.9507 - val_acc: 0.0660\n",
      "Epoch 2/4\n",
      "341/341 [==============================] - 46s - loss: 0.1949 - acc: 0.9589 - val_loss: 4.5569 - val_acc: 0.0943\n",
      "Epoch 3/4\n",
      "341/341 [==============================] - 46s - loss: 0.1011 - acc: 0.9853 - val_loss: 3.6119 - val_acc: 0.0849\n",
      "Epoch 4/4\n",
      "341/341 [==============================] - 46s - loss: 0.0927 - acc: 0.9736 - val_loss: 3.2228 - val_acc: 0.0943\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "341/341 [==============================] - 49s - loss: 2.6612 - acc: 0.2434 - val_loss: 10.6376 - val_acc: 0.0566\n",
      "Epoch 2/2\n",
      "341/341 [==============================] - 49s - loss: 1.5988 - acc: 0.4516 - val_loss: 7.5132 - val_acc: 0.1038\n",
      "Epoch 1/4\n",
      "341/341 [==============================] - 48s - loss: 1.2338 - acc: 0.6246 - val_loss: 4.7026 - val_acc: 0.1226\n",
      "Epoch 2/4\n",
      "341/341 [==============================] - 45s - loss: 1.1855 - acc: 0.6276 - val_loss: 3.5008 - val_acc: 0.0943\n",
      "Epoch 3/4\n",
      "341/341 [==============================] - 46s - loss: 0.9777 - acc: 0.7067 - val_loss: 2.9607 - val_acc: 0.1132\n",
      "Epoch 4/4\n",
      "341/341 [==============================] - 46s - loss: 0.9214 - acc: 0.7097 - val_loss: 2.7641 - val_acc: 0.0943\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "341/341 [==============================] - 48s - loss: 0.8498 - acc: 0.7185 - val_loss: 2.6332 - val_acc: 0.0755\n",
      "Epoch 2/15\n",
      "341/341 [==============================] - 46s - loss: 0.7837 - acc: 0.7537 - val_loss: 2.5576 - val_acc: 0.1038\n",
      "Epoch 3/15\n",
      "341/341 [==============================] - 48s - loss: 0.7104 - acc: 0.7625 - val_loss: 2.5157 - val_acc: 0.0943\n",
      "Epoch 4/15\n",
      "341/341 [==============================] - 46s - loss: 0.6497 - acc: 0.8065 - val_loss: 2.4645 - val_acc: 0.1981\n",
      "Epoch 5/15\n",
      "341/341 [==============================] - 49s - loss: 0.6288 - acc: 0.8299 - val_loss: 2.3930 - val_acc: 0.2642\n",
      "Epoch 6/15\n",
      "341/341 [==============================] - 45s - loss: 0.5408 - acc: 0.8622 - val_loss: 2.3373 - val_acc: 0.2830\n",
      "Epoch 7/15\n",
      "341/341 [==============================] - 49s - loss: 0.5816 - acc: 0.8328 - val_loss: 2.3499 - val_acc: 0.2358\n",
      "Epoch 8/15\n",
      "341/341 [==============================] - 49s - loss: 0.4894 - acc: 0.8592 - val_loss: 2.3886 - val_acc: 0.1981\n",
      "Epoch 9/15\n",
      "341/341 [==============================] - 46s - loss: 0.5058 - acc: 0.8328 - val_loss: 2.3848 - val_acc: 0.1887\n",
      "Epoch 10/15\n",
      "341/341 [==============================] - 46s - loss: 0.4483 - acc: 0.8680 - val_loss: 2.4084 - val_acc: 0.2075\n",
      "Epoch 11/15\n",
      "341/341 [==============================] - 46s - loss: 0.4669 - acc: 0.8622 - val_loss: 2.4617 - val_acc: 0.2264\n",
      "Epoch 12/15\n",
      "341/341 [==============================] - 46s - loss: 0.4415 - acc: 0.8622 - val_loss: 2.4552 - val_acc: 0.2358\n",
      "Epoch 13/15\n",
      "341/341 [==============================] - 46s - loss: 0.3909 - acc: 0.8915 - val_loss: 2.4256 - val_acc: 0.2358\n",
      "Epoch 14/15\n",
      "341/341 [==============================] - 46s - loss: 0.3879 - acc: 0.9003 - val_loss: 2.4132 - val_acc: 0.2547\n",
      "Epoch 15/15\n",
      "341/341 [==============================] - 46s - loss: 0.3970 - acc: 0.8974 - val_loss: 2.3954 - val_acc: 0.2736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb18414588>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=15, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(13, activation='softmax')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "341/341 [==============================] - 72s - loss: 3.7331 - acc: 0.0909 - val_loss: 9.4050 - val_acc: 0.0566\n",
      "Epoch 2/2\n",
      "341/341 [==============================] - 68s - loss: 2.8677 - acc: 0.2141 - val_loss: 7.4230 - val_acc: 0.0566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcac46e3f98>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "341/341 [==============================] - 70s - loss: 2.7840 - acc: 0.2610 - val_loss: 5.0322 - val_acc: 0.0566\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 68s - loss: 2.7754 - acc: 0.2317 - val_loss: 3.9098 - val_acc: 0.0849\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 68s - loss: 2.4673 - acc: 0.2845 - val_loss: 3.3253 - val_acc: 0.0566\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 68s - loss: 2.4306 - acc: 0.2610 - val_loss: 3.0890 - val_acc: 0.0755\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 68s - loss: 2.3948 - acc: 0.3050 - val_loss: 2.9492 - val_acc: 0.0849\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 68s - loss: 2.2402 - acc: 0.2933 - val_loss: 2.8968 - val_acc: 0.0660\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 68s - loss: 2.1393 - acc: 0.3460 - val_loss: 2.8892 - val_acc: 0.0660\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 68s - loss: 2.1700 - acc: 0.3460 - val_loss: 2.9317 - val_acc: 0.0849\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 68s - loss: 2.1064 - acc: 0.4047 - val_loss: 3.0085 - val_acc: 0.1226\n",
      "Epoch 10/10\n",
      "341/341 [==============================] - 68s - loss: 2.0254 - acc: 0.3871 - val_loss: 3.1325 - val_acc: 0.0943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcae8739828>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "341/341 [==============================] - 70s - loss: 1.9165 - acc: 0.4018 - val_loss: 3.2197 - val_acc: 0.0849\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 68s - loss: 1.9480 - acc: 0.4018 - val_loss: 3.2974 - val_acc: 0.0755\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 68s - loss: 1.9390 - acc: 0.4076 - val_loss: 3.4499 - val_acc: 0.0755\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 68s - loss: 1.8501 - acc: 0.4370 - val_loss: 3.6404 - val_acc: 0.0660\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 67s - loss: 1.8633 - acc: 0.4340 - val_loss: 3.8762 - val_acc: 0.0660\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 68s - loss: 1.6042 - acc: 0.4956 - val_loss: 3.9929 - val_acc: 0.0660\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 68s - loss: 1.8922 - acc: 0.4487 - val_loss: 4.0707 - val_acc: 0.0660\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 68s - loss: 1.7096 - acc: 0.4428 - val_loss: 4.0971 - val_acc: 0.0660\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 68s - loss: 1.6784 - acc: 0.4692 - val_loss: 4.1507 - val_acc: 0.0755\n",
      "Epoch 10/10\n",
      "341/341 [==============================] - 68s - loss: 1.7069 - acc: 0.4692 - val_loss: 4.2706 - val_acc: 0.0755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcac43d1ef0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n",
      "Found 106 images belonging to 13 classes.\n",
      "Found 4 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 512, 14, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(13, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 341 samples, validate on 106 samples\n",
      "Epoch 1/1\n",
      "341/341 [==============================] - 0s - loss: 5.1119 - acc: 0.0704 - val_loss: 5.8372 - val_acc: 0.1038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f413c5e4e80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 341 samples, validate on 106 samples\n",
      "Epoch 1/2\n",
      "341/341 [==============================] - 0s - loss: 4.3962 - acc: 0.1232 - val_loss: 4.0590 - val_acc: 0.1698\n",
      "Epoch 2/2\n",
      "341/341 [==============================] - 0s - loss: 3.8803 - acc: 0.1437 - val_loss: 3.4980 - val_acc: 0.1415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f413c48a908>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "da_batches = get_batches(path+'train', gen_t, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = conv_model.predict_generator(da_batches, da_batches.nb_sample*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/da_conv_feat2.dat', da_conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = load_array(path+'results/da_conv_feat2.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = np.concatenate([da_conv_feat, conv_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_trn_labels = np.concatenate([trn_labels]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(13, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_da_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2046 samples, validate on 106 samples\n",
      "Epoch 1/1\n",
      "2046/2046 [==============================] - 2s - loss: 4.7986 - acc: 0.0929 - val_loss: 2.4010 - val_acc: 0.3019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f413c3cc6d8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2046 samples, validate on 106 samples\n",
      "Epoch 1/4\n",
      "2046/2046 [==============================] - 2s - loss: 3.9156 - acc: 0.1554 - val_loss: 1.9257 - val_acc: 0.3962\n",
      "Epoch 2/4\n",
      "2046/2046 [==============================] - 2s - loss: 3.5523 - acc: 0.1760 - val_loss: 1.7215 - val_acc: 0.3868\n",
      "Epoch 3/4\n",
      "2046/2046 [==============================] - 2s - loss: 3.2001 - acc: 0.2165 - val_loss: 1.5512 - val_acc: 0.4151\n",
      "Epoch 4/4\n",
      "2046/2046 [==============================] - 2s - loss: 2.9488 - acc: 0.2322 - val_loss: 1.5125 - val_acc: 0.4811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f413c49a4e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2046 samples, validate on 106 samples\n",
      "Epoch 1/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.7239 - acc: 0.2722 - val_loss: 1.4520 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.5383 - acc: 0.3050 - val_loss: 1.3897 - val_acc: 0.5660\n",
      "Epoch 3/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.3959 - acc: 0.3226 - val_loss: 1.3809 - val_acc: 0.5283\n",
      "Epoch 4/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.2891 - acc: 0.3441 - val_loss: 1.3242 - val_acc: 0.6132\n",
      "Epoch 5/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.1800 - acc: 0.3534 - val_loss: 1.3237 - val_acc: 0.6038\n",
      "Epoch 6/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.0407 - acc: 0.3978 - val_loss: 1.3015 - val_acc: 0.5943\n",
      "Epoch 7/10\n",
      "2046/2046 [==============================] - 2s - loss: 2.0477 - acc: 0.4008 - val_loss: 1.2748 - val_acc: 0.6415\n",
      "Epoch 8/10\n",
      "2046/2046 [==============================] - 2s - loss: 1.8966 - acc: 0.4306 - val_loss: 1.2606 - val_acc: 0.6415\n",
      "Epoch 9/10\n",
      "2046/2046 [==============================] - 2s - loss: 1.8625 - acc: 0.4257 - val_loss: 1.2653 - val_acc: 0.6415\n",
      "Epoch 10/10\n",
      "2046/2046 [==============================] - 2s - loss: 1.7551 - acc: 0.4814 - val_loss: 1.2290 - val_acc: 0.6226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4137d258d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=10, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pseudo = bn_model.predict(conv_val_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([da_trn_labels, val_pseudo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_feat = np.concatenate([da_conv_feat, conv_val_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2152 samples, validate on 106 samples\n",
      "Epoch 1/1\n",
      "2152/2152 [==============================] - 2s - loss: 1.7460 - acc: 0.4763 - val_loss: 1.2027 - val_acc: 0.6604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4137d34940>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2152 samples, validate on 106 samples\n",
      "Epoch 1/4\n",
      "2152/2152 [==============================] - 2s - loss: 1.6952 - acc: 0.4912 - val_loss: 1.1950 - val_acc: 0.6415\n",
      "Epoch 2/4\n",
      "2152/2152 [==============================] - 2s - loss: 1.6812 - acc: 0.5121 - val_loss: 1.1925 - val_acc: 0.6321\n",
      "Epoch 3/4\n",
      "2152/2152 [==============================] - 2s - loss: 1.6243 - acc: 0.5135 - val_loss: 1.1976 - val_acc: 0.6321\n",
      "Epoch 4/4\n",
      "2152/2152 [==============================] - 2s - loss: 1.5106 - acc: 0.5516 - val_loss: 1.1769 - val_acc: 0.6604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4137d34860>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2152 samples, validate on 106 samples\n",
      "Epoch 1/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.5225 - acc: 0.5469 - val_loss: 1.1643 - val_acc: 0.6604\n",
      "Epoch 2/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.4463 - acc: 0.5599 - val_loss: 1.1618 - val_acc: 0.6604\n",
      "Epoch 3/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.4365 - acc: 0.5743 - val_loss: 1.1793 - val_acc: 0.6509\n",
      "Epoch 4/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.3480 - acc: 0.5994 - val_loss: 1.1572 - val_acc: 0.6415\n",
      "Epoch 5/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.3568 - acc: 0.6004 - val_loss: 1.1518 - val_acc: 0.6698\n",
      "Epoch 6/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.3329 - acc: 0.6138 - val_loss: 1.1377 - val_acc: 0.6698\n",
      "Epoch 7/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.3031 - acc: 0.6241 - val_loss: 1.1236 - val_acc: 0.6792\n",
      "Epoch 8/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.2818 - acc: 0.6306 - val_loss: 1.1050 - val_acc: 0.6792\n",
      "Epoch 9/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.2427 - acc: 0.6348 - val_loss: 1.0868 - val_acc: 0.6887\n",
      "Epoch 10/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.2384 - acc: 0.6408 - val_loss: 1.0972 - val_acc: 0.6887\n",
      "Epoch 11/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.2461 - acc: 0.6417 - val_loss: 1.0854 - val_acc: 0.7170\n",
      "Epoch 12/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.1806 - acc: 0.6612 - val_loss: 1.0762 - val_acc: 0.7170\n",
      "Epoch 13/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.1893 - acc: 0.6757 - val_loss: 1.0722 - val_acc: 0.7264\n",
      "Epoch 14/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.1507 - acc: 0.6766 - val_loss: 1.0678 - val_acc: 0.7170\n",
      "Epoch 15/15\n",
      "2152/2152 [==============================] - 2s - loss: 1.1610 - acc: 0.6770 - val_loss: 1.0640 - val_acc: 0.7170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4137d34518>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(comb_feat, comb_pseudo, batch_size=batch_size, nb_epoch=15, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = bn_model.to_json()\n",
    "with open(\"bn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "bn_model.save_weights(path+'models/bn-ps8.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "path = \"data/\"\n",
    "import importlib as imp\n",
    "import utils; imp.reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('bn_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(path+'models/bn-ps8.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict_classes(conv_test_feat, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 7 2 4]\n"
     ]
    }
   ],
   "source": [
    "print (preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - aspergillus ear rot\n",
    "2 - bacterial stalk rot\n",
    "3 - common rust\n",
    "4 - corn leaf blight\n",
    "5 - eyespot\n",
    "6 - gray leaf spot\n",
    "7 - nematode\n",
    "8 - nigrospora ear rot\n",
    "9 - penicillium ear rot\n",
    "10 - purple corn syndrome\n",
    "11 - red root rot\n",
    "12 - rootless corn syndrome\n",
    "13 - seedling blight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
